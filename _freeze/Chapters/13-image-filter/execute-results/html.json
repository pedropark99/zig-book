{
  "hash": "3c079d9adcbe811d8a7f9a2ce7d087ac",
  "result": {
    "engine": "knitr",
    "markdown": "---\nengine: knitr\nknitr: true\nsyntax-definition: \"../Assets/zig.xml\"\n---\n\n\n\n\n\n# Project 4 - Developing an image filter\n\nIn this chapter we are going to build a new project. The objective of\nthis project is to write a program that applies a filter over an image.\nMore specifically, a \"grayscale filter\", which transforms\nany color image into a grayscale image.\n\nWe are going to use the image displayed in @fig-pascal in this project.\nIn other words, we want to transform this colored image into a grayscale image,\nby using our \"image filter program\" written in Zig.\n\n![A photo of the chilean-american actor Pedro Pascal. Source: Google Images.](../ZigExamples/image_filter/pedro_pascal.png){#fig-pascal}\n\nWe don't need to write a lot of code to build such \"image filter program\". However, we first need\nto understand how digital images work. That is why we begin this chapter\nby explaining the theory behind digital images and how colors are represented in modern computers.\nWe also give a brief explanation about the PNG (Portable Network Graphics) file format, which is the format used\nin the example images.\n\nAt the end of this chapter, we should have a full example of a program that takes the PNG image displayed in @fig-pascal\nas input, and writes a new image to the current working directory that is the grayscale version of this input image.\nThis grayscale version of @fig-pascal is exposed in @fig-pascal-gray.\nYou can find the full source code of this small project at the `ZigExamples/image_filter`\n[folder at the official repository of this book](https://github.com/pedropark99/zig-book/tree/main/ZigExamples/image_filter)[^img-filter-folder].\n\n\n![The grayscale version of the photo.](../ZigExamples/image_filter/pedro_pascal_filter.png){#fig-pascal-gray}\n\n\n## How we see things? {#sec-eyes}\n\nIn this section, I want to briefly describe to you how we (humans) actually see things with our own eyes.\nI mean, how our eyes work? If you do have a very basic understanding of how our eyes work, you will understand\nmore easily how digital images are made. Because the techniques behind digital images\nwere developed by taking a lot of inspiration from how our human eyes work.\n\nYou can interpret a human eye as a light sensor, or, a light receptor. The eye receives some amount of light as input,\nand it interprets the colors that are present in this \"amount of light\".\nIf no amount of light hits the eye, then, the eye cannot extract color from it, and as result,\nwe end up seeing nothing, or, more precisely, we see complete blackness.\n\nTherefore, everything depends on light. What we actually see are the colors (blue, red, orange, green, purple, yellow, etc.) that\nare being reflected from the light that is hitting our eyes. **Light is the source of all colors!**\nThis is what Isaac Newton discovered on his famous prism experiment[^newton] in the 1660s.\n\n[^newton]: <https://library.si.edu/exhibition/color-in-a-new-light/science>\n\nInside our eyes, we have a specific type of cell called the \"cone cell\".\nOur eye have three different types, or, three different versions of these \"cone cells\".\nEach type of cone cell is very sensitive to a specific spectrum of the light. More specifically,\nto the spectrums that define the colors red, green and blue.\nSo, in summary, our eyes have specific types of cells that\nare highly sensitive to these three colors (red, green and blue).\n\nThese are the cells responsible for perceiving the color present in the light that hits our eyes.\nAs a result, our eyes perceives color as a mixture of these three colors (red, green and blue). By having an amount\nof each one of these three colors, and mixing them together, we can get any other visible color\nthat we want. So every color that we see is perceived as a specific mixture of blues, greens and reds,\nlike 30% of red, plus 20% of green, plus 50% of blue.\n\nWhen these cone cells perceive (or, detect) the colors that are found in the\nlight that is hitting our eyes, these cells produce electrical signals, which are sent to the brain.\nOur brain interprets these electrical signals, and use them to form the image that we are seeing\ninside our head.\n\nBased on what we have discussed here, the bullet points exposed below describes the sequence of events that\ncomposes this very simplified version of how our human eyes work:\n\n1. Light hits our eyes.\n1. The cone cells perceive the colors that are present in this light.\n1. Cone cells produce electrical signals that describes the colors that were perceived in the light.\n1. The electrical signals are sent to the brain.\n1. Brain interprets these signals, and form the image based on the colors identified by these electrical signals.\n\n\n## How digital images work? {#sec-digital-img}\n\nA digital image is a \"digital representation\" of an image that we see with our eyes.\nIn other words, a digital image is a \"digital representation\" of the colors that we see\nand perceive through the light.\nIn the digital world, we have two types of images, which are: vector images and raster images.\nVector images are not described here. So just remember that the content discussed here\n**is related solely to raster images**, and not vector images.\n\nA raster image is a type of digital image that is represented as a 2D (two dimensional) matrix\nof pixels. In other words, every raster image is basically a rectangle of pixels, and each pixel have a particular color.\nSo, a raster image is just a rectangle of pixels, and each of these pixels are displayed in the screen of your computer (or the screen\nof any other device, e.g. laptop, tablet, smartphone, etc.) as a color.\n\n@fig-raster demonstrates this idea. If you take any raster image, and you zoom into it very hard,\nyou will see the actual pixels of the image. JPEG, TIFF and PNG are file formats that are commonly\nused to store raster images.\n\n![Zooming over a raster image to see the pixels. Source: Google Images.](../Figures/imagem-raster.png){#fig-raster}\n\nThe more pixels the image has, the more information and detail we can include in the image.\nThe more accurate, sharp and pretty the image will look. This is why photographic cameras\nusually produce big raster images, with several megapixels of resolution, to include as much detail as possible into the final image.\nAs an example, a digital image with dimensions of 1920 pixels wide and 1080 pixels high, would be a image that\ncontains $1920 \\times 1080 = 2073600$ pixels in total. You could also say that the \"total area\" of the image is\nof 2073600 pixels, although the concept of \"area\" is not really used here in computer graphics.\n\nMost digital images we see in our modern world uses the RGB color model. RGB stands for (red, green and blue).\nSo the color of each pixel in these raster images are usually represented as a mixture of red, green and blue,\njust like in our eyes. That is, the color of each pixel is identified by a set of\nthree different integer values. Each integer value identifies the \"amount\" of each color (red, green and blue).\nFor example, the set `(199, 78, 70)` identifies a color that is more close to red. We have 199 of red, 78 of green,\nand 70 of blue. In contrast, the set `(129, 77, 250)` describes a color that is more close to purple. Et cetera.\n\n\n\n### Images are displayed from top to bottom\n\nThis is not a rule written in stone, but the big majority of digital images are displayed from top\nto bottom and left to right. Most computers screens also follow this pattern. So, the first pixels\nin the image are the ones that are at the top and left corner of the image. You can find a visual representation\nof this logic in @fig-img-display.\n\nAlso notice in @fig-img-display that, because a raster image is essentially a 2D matrix of pixels,\nthe image is organized into rows and columns of pixels. The columns are defined by the horizontal x axis,\nwhile the rows are defined by the vertical y axis.\n\nEach pixel (i.e., the gray rectangles) exposed in @fig-img-display contains a number inside of it.\nThese numbers are the indexes of the pixels. You can notice that the first pixels are in the top and left\ncorner, and also, that the indexes of these pixels \"grow to the sides\", or, in other words, they grow in the direction of the horizontal x axis.\nMost raster images are organized as rows of pixels. Thus, when these digital images are\ndisplayed, the screen display the first row of pixels, then, the second row, then, the third row, etc.\n\n![How the pixels of raster images are displayed.](./../Figures/image-display.png){#fig-img-display}\n\n\n\n\n\n\n### Representing the matrix of pixels in code {#sec-pixel-repr}\n\nOk, we know already that raster images are represented as 2D matrices of pixels.\nBut we do not have a notion of a 2D matrix in Zig. Actually, most low-level languages in general\n(Zig, C, Rust, etc.) do not have such notion.\nSo how can we represent such matrix of pixels in Zig, or any other low-level language?\nThe strategy that most programmers choose in this situation is to just use a normal 1D array to store the values of\nthis 2D matrix. In other words, you just create an normal 1D array, and store all values from both dimensions into this 1D array.\n\nAs an example, suppose we have a very small image of dimensions 4x3.\nSince a raster image is represented as a 2D matrix of pixels, and each pixel\nis represented by 3 \"unsigned 8-bit\" integer values, we have 12 pixels in\ntotal in this image, which are represented by $3 \\times 12 = 36$ integer values.\nTherefore, we need to create an array of 36 `u8` values to store this small image.\n\nThe reason why unsigned 8-bit integer (`u8`) values are used to represent the amounts of each color,\ninstead of any other integer type, is because they take the minimum amount of space as possible, or,\nthe minimum amount of bits as possible. Which helps to reduces the binary size of the image, i.e., the 2D matrix.\nAlso, they convey a good amount of precision and detail about the colors, even though they can represent\na relatively small range (from 0 to 255) of \"color amounts\".\n\nComing back to our initial example of a 4x3 image,\nthe `matrix` object exposed below could be an example of an 1D array that stores\nthe data that represents this 4x3 image.\n\n\n::: {.cell}\n\n```{.zig .cell-code}\nconst matrix = [_]u8{\n    201, 10, 25, 185, 65, 70,\n    65, 120, 110, 65, 120, 117,\n    98, 95, 12, 213, 26, 88,\n    143, 112, 65, 97, 99, 205,\n    234, 105, 56, 43, 44, 216,\n    45, 59, 243, 211, 209, 54,\n};\n```\n:::\n\n\nThe first three integer values in this array are the color amounts of the first pixel in the image.\nThe next three integers are the colors amounts for the second pixel.\nAnd the sequence goes on in this pattern. Having that in mind, the size of the array that stores\na raster image is usually a multiple of 3. In this case, the array have a size of 36.\n\nI mean, the size of the array is **usually** a multiple of 3, because in specific circumstances,\nit can also be a multiple of 4. This happens when a transparency amount is\nalso included into the raster image. In other words, there are some types of raster images\nthat uses a different color model, which is the RGBA (red, green, blue and alpha)\ncolor model. The \"alpha\" corresponds to an amount of transparency in the pixel.\nSo every pixel in a RGBA image is represented by a red, green, blue and alpha values.\n\nMost raster images uses the standard RGB model, so, for the most part, you will\nsee arrays sizes that are multiples of 3. But some images, especially the ones\nthat are stored in PNG files, might be using the RGBA model, and, therefore, are\nrepresented by an array whose size is a multiple of 4.\n\nIn our case here, the example image of our project (@fig-pascal) is a raster image\nstored in a PNG file, and this specific image is using the RGBA color model. Therefore,\neach pixel in the image is represented by 4 different integer values, and, as consequence,\nto store this image in our Zig code, we need to create an array whose size is a multiple of 4.\n\n\n## The PNG library that we are going to use\n\nLet's begin our project by focusing on writing the necessary Zig code to\nread the data from the PNG file. In other words, we want to read the PNG file exposed\nin @fig-pascal, and parse its data to extract the 2D matrix of pixels that represents the image.\n\nAs we have discussed in @sec-pixel-repr, the image that we are using as example here\nis a PNG file that uses the RGBA color model, and, therefore, each pixel of the image\nis represented by 4 integer values. You can download this image by visiting the `ZigExamples/image_filter`\n[folder at the official repository of this book](https://github.com/pedropark99/zig-book/tree/main/ZigExamples/image_filter)[^img-filter-folder].\nYou can also find in this folder the complete source code of this small project that we\nare developing here.\n\n[^img-filter-folder]: <https://github.com/pedropark99/zig-book/tree/main/ZigExamples/image_filter>\n\nThere are some C libraries available that we can use to read and parse PNG files.\nThe most famous and used of all is `libpng`, which is the \"official library\" for reading and writing\nPNG files. Although this library is available on most operating system, it's well known\nfor being complex and hard to use.\n\nThat is why, I'm going to use a more modern alternative here in this project, which is the `libspng` library.\nI choose to use this C library here, because it's much, much simpler to use than `libpng`,\nand it also offers very good performance for all operations. You can checkout the\n[official website of the library](https://libspng.org/)[^libspng]\nto know more about it. You will also find there some documentation that might help you to understand and\nfollow the code examples exposed here.\n\n[^libspng]: <https://libspng.org/>\n\n\nFirst of all, remember to build and install this `libspng` into your system. Because\nif you don't do this step, the `zig` compiler will not be able to find the files and resources of\nthis library in your computer, and link them with the Zig source code that we are writing together here.\nThere is good information about how to build and install the library at the\n[build section of the library documentation](https://libspng.org/docs/build/)[^lib-build].\n\n[^lib-build]: <https://libspng.org/docs/build/>\n\n\n\n\n## Reading the PNG file\n\nIn order to extract the pixel data from the PNG file, we need to read and decode the file.\nA PNG file is just a binary file written in the \"PNG format\". Luckily, the `libspng` library offers\na function called `spng_decode_image()` that does all this heavy work for us.\n\nNow, since `libspng` is a C library, most of the file and I/O operations in this library are made by using\na `FILE` C pointer. Because of that, is probably a better idea to use the `fopen()` C function\nto open our PNG file, instead of using the `openFile()` method that I introduced in @sec-filesystem.\nThat is why I'm importing the `stdio.h` C header in this project, and using the `fopen()` C function to open the file.\n\nIf you look at the snippet below, you can see that we are:\n\n1. opening the PNG file with `fopen()`.\n1. creating the `libspng` context with `spng_ctx_new()`.\n1. using `spng_set_png_file()` to specify the `FILE` object that reads the PNG file that we are going to use.\n\nEvery operation in `libspng` is made through a \"context object\". In our snippet below, this object is `ctx`.\nAlso, to perform an operation over a PNG file, we need to specify which exact PNG file we are referring to.\nThis is the job of `spng_set_png_file()`. We are using this function to specify the file descriptor\nobject that reads the PNG file that we want to use.\n\n\n\n::: {.cell}\n\n```{.zig .cell-code}\nconst c = @cImport({\n    @cDefine(\"_NO_CRT_STDIO_INLINE\", \"1\");\n    @cInclude(\"stdio.h\");\n    @cInclude(\"spng.h\");\n});\n\nconst path = \"pedro_pascal.png\";\nconst file_descriptor = c.fopen(path, \"rb\");\nif (file_descriptor == null) {\n    @panic(\"Could not open file!\");\n}\nconst ctx = c.spng_ctx_new(0) orelse unreachable;\n_ = c.spng_set_png_file(\n    ctx, @ptrCast(file_descriptor)\n);\n```\n:::\n\n\nBefore we continue, is important to emphasize the following: since we have opened the file with `fopen()`,\nwe have to remember to close the file at the end of the program, with `fclose()`.\nIn other words, after we have done everything that we wanted to do with the PNG file\n`pedro_pascal.png`, we need to close this file, by applying `fclose()` over the file descriptor object.\nWe could use also the `defer` keyword to help us in this task, if we want to.\nThis code snippet below demonstrates this step:\n\n\n::: {.cell}\n\n```{.zig .cell-code}\nif (c.fclose(file_descriptor) != 0) {\n    return error.CouldNotCloseFileDescriptor;\n}\n```\n:::\n\n\n\n\n\n### Reading the image header section\n\nNow, the context object `ctx` is aware of our PNG file `pedro_pascal.png`, because it has access to\na file descriptor object to this file. The first thing that we are going to do is to read the\n\"image header section\" of the PNG file. This \"image header section\" is the section\nof the file that contains some basic information about the PNG file, like, the bit depth of the pixel data\nof the image, the color model used in the file, the dimensions of the image (height and width in number of pixels),\netc.\n\nTo make things easier, I will encapsulate this \"read image header\" operation into a\nnice and small function called `get_image_header()`. All that this function needs to do\nis to call the `spng_get_ihdr()` function. This function from `libspng` is responsible\nfor reading the image header data, and storing it into a C struct named `spng_ihdr`.\nThus, an object of type `spng_ihdr` is a C struct that contains the data from the\nimage header section of the PNG file.\n\nSince this Zig function is receiving a C object (the `libspng` context object) as input, I marked\nthe function argument `ctx` as \"a pointer to the context object\" (`*c.spng_ctx`), following the recommendations\nthat we have discussed in @sec-pass-c-structs.\n\n\n::: {.cell}\n\n```{.zig .cell-code}\nfn get_image_header(ctx: *c.spng_ctx) !c.spng_ihdr {\n    var image_header: c.spng_ihdr = undefined;\n    if (c.spng_get_ihdr(ctx, &image_header) != 0) {\n        return error.CouldNotGetImageHeader;\n    }\n\n    return image_header;\n}\n\nvar image_header = try get_image_header(ctx);\n```\n:::\n\n\nAlso notice in this function, that I'm checking if the `spng_get_ihdr()` function call have\nreturned or not an integer value that is different than zero. Most functions from the\n`libspng` library return a code status as result, and the code status \"zero\" means\n\"success\". So any code status that is different than zero means that an error\noccurred while running `spng_get_ihdr()`. This is why I'm returning an error value from\nthe function in case the code status returned by the function is different than zero.\n\n\n### Allocating space for the pixel data\n\nBefore we read the pixel data from the PNG file, we need to allocate enough space to hold this data.\nBut in order to allocate such space, we first need to know how much space we need to allocate.\nThe dimensions of the image are obviously needed to calculate the size of this space. But there are\nother elements that also affect this number, such as the color model used in the image, the bit depth, and others.\n\nAnyway, all of this means that calculating the size of the space that we need, is not a simple task.\nThat is why the `libspng` library offers an utility function named\n`spng_decoded_image_size()` to calculate this size for us. Once again, I'm going\nto encapsulate the logic around this C function into a nice and small Zig function\nnamed `calc_output_size()`. You can see below that this function returns a nice\ninteger value as result, informing the size of the space that we need to allocate.\n\n\n\n::: {.cell}\n\n```{.zig .cell-code}\nfn calc_output_size(ctx: *c.spng_ctx) !u64 {\n    var output_size: u64 = 0;\n    const status = c.spng_decoded_image_size(\n        ctx, c.SPNG_FMT_RGBA8, &output_size\n    );\n    if (status != 0) {\n        return error.CouldNotCalcOutputSize;\n    }\n    return output_size;\n}\n```\n:::\n\n\n\n\nYou might quest yourself what the value `SPNG_FMT_RGBA8` means. This value is actually an enum\nvalue defined in the `spng.h` C header file. This enum is used to identify a \"PNG format\".\nMore precisely, it identifies a PNG file that uses the RGBA color model and 8 bit depth.\nSo, by providing this enum value as input to the `spng_decoded_image_size()` function,\nwe are saying to this function to calculate the size of the decoded pixel data, by considering\na PNG file that follows this \"RGBA color model with 8 bit depth\" format.\n\nHaving this function, we can use it in conjunction with an allocator object, to allocate an\narray of bytes (`u8` values) that is big enough to store the decoded pixel data of the image.\nNotice that I'm using `@memset()` to initialize the entire array to zero.\n\n\n::: {.cell}\n\n```{.zig .cell-code}\nconst output_size = try calc_output_size(ctx);\nvar buffer = try allocator.alloc(u8, output_size);\n@memset(buffer[0..], 0);\n```\n:::\n\n\n\n### Decoding the image data\n\nNow that we have the necessary space to store the decoded pixel data of the image,\nwe can start to actually decode and extract this pixel data from the image,\nby using the `spng_decode_image()` C function.\n\nThe `read_data_to_buffer()` Zig function exposed below summarises the necessary\nsteps to read this decoded pixel data, and store it into an input buffer.\nNotice that this function is encapsulating the logic around the `spng_decode_image()` function.\nAlso, we are using the `SPNG_FMT_RGBA8` enum value once again to inform the corresponding function,\nthat the PNG image being decoded, uses the RGBA color model and 8 bit depth.\n\n\n::: {.cell}\n\n```{.zig .cell-code}\nfn read_data_to_buffer(ctx: *c.spng_ctx, buffer: []u8) !void {\n    const status = c.spng_decode_image(\n        ctx,\n        buffer.ptr,\n        buffer.len,\n        c.SPNG_FMT_RGBA8,\n        0\n    );\n\n    if (status != 0) {\n        return error.CouldNotDecodeImage;\n    }\n}\n```\n:::\n\n\nHaving this function at hand, we can apply it over our context object, and also, over\nthe buffer object that we have allocated in the previous section to hold the decoded pixel data\nof the image:\n\n\n::: {.cell}\n\n```{.zig .cell-code}\ntry read_data_to_buffer(ctx, buffer[0..]);\n```\n:::\n\n\n\n### Looking at the pixel data\n\nNow that we have the pixel data stored in our \"buffer object\", we can take\na quick look at the bytes. In the example below, we are looking at the first\n12 bytes in the decoded pixel data.\n\nIf you take a close look at these values, you might notice that every 4 bytes\nin the sequence is 255. Which, coincidentally is the maximum possible integer value\nto be represented by a `u8` value. So, if the range from 0 to 255, which is the range\nof integer values that can be represented by an `u8` value, can be represented as a scale from 0% to 100%,\nthese 255 values are essentially 100% in that scale.\n\nIf you recall from @sec-pixel-repr, I have\ndescribed in that section that our `pedro_pascal.png` PNG file uses the RGBA color model,\nwhich adds an alpha (or transparency) byte to each pixel in the image.\nAs consequence, each pixel in the image is represented by 4 bytes. Since we are looking\nhere are the first 12 bytes in the image, it means that we are looking at the data from\nthe first $12 / 4 = 3$ pixels in the image.\n\nSo, based on how these first 12 bytes (or these 3 pixels) look, with these 255 values at every 4 bytes, we can say that is likely\nthat every pixel in the image have alpha (or transparency) setted to 100%. This might not be true,\nbut, is the most likely possibility. Also, if we look at the image itself, which if your recall is\nexposed in @fig-pascal, we can see that the transparency does not change across the image,\nwhich enforces this theory.\n\n\n\n::: {.cell}\n\n```{.zig .cell-code}\ntry stdout.print(\"{any}\\n\", .{buffer[0..12]});\ntry stdout.flush();\n```\n:::\n\n\n```\n{\n    200, 194, 216, 255, 203, 197,\n    219, 255, 206, 200, 223, 255\n}\n```\n\n\nWe can see in the above result that the first pixel in this image have 200 of red, 194 of green, and 216 of blue.\nHow do I know the order in which the colors appears in the sequence? If you have not guessed that yet,\nis because of the acronym RGB. First RED, then GREEN, then BLUE. If we scale these integer values\naccording to our scale of 0% to 100% (0 to 255), we get 78% of red, 76% of green and 85% of blue.\n\n\n\n## Applying the image filter\n\nNow that we have the data of each pixel in the image, we can focus on applying our image\nfilter over these pixels. Remember, our objective here is to apply a grayscale filter over\nthe image. A grayscale filter is a filter that transforms a colored image into a grayscale image.\n\nThere are different formulas and strategies to transform a colored image into a grayscale image.\nBut all of these different strategies normally involve applying some math over the colors of each pixel.\nIn this project, we are going to use the most general formula, which is exposed below.\nThis formula considers $r$ as the red of the pixel, $g$ as the green, $b$ as the blue, and $p'$ as the\nlinear luminance of the pixel.\n\n$$\n    p' = (0.2126 \\times r) + (0.7152 \\times g) + (0.0722 \\times b)\n$${#eq-grayscale}\n\nThis @eq-grayscale is the formula to calculate the linear luminance of a pixel. It's worth noting that this formula\nworks only for images whose pixels are using the sRGB color space, which is the standard color space\nfor the web. Thus, ideally, all images on the web should use this color space. Luckily,\nthis is our case here, i.e., the `pedro_pascal.png` image is using this sRGB color space, and, as consequence,\nwe can use the @eq-grayscale. You can read more about this formula at the Wikipedia page for grayscale [@wiki_grayscale].\n\nThe `apply_image_filter()` function exposed below summarises the necessary steps to\napply @eq-grayscale over the pixels in the image. We just apply this function\nover our buffer object that contains our pixel data, and, as result, the pixel\ndata stored in this buffer object should now represent the grayscale version of our image.\n\n\n::: {.cell}\n\n```{.zig .cell-code}\nfn apply_image_filter(buffer:[]u8) !void {\n    const len = buffer.len;\n    const red_factor: f16 = 0.2126;\n    const green_factor: f16 = 0.7152;\n    const blue_factor: f16 = 0.0722;\n    var index: u64 = 0;\n    while (index < len) : (index += 4) {\n        const rf: f16 = @floatFromInt(buffer[index]);\n        const gf: f16 = @floatFromInt(buffer[index + 1]);\n        const bf: f16 = @floatFromInt(buffer[index + 2]);\n        const y_linear: f16 = (\n            (rf * red_factor) + (gf * green_factor)\n            + (bf * blue_factor)\n        );\n        buffer[index] = @intFromFloat(y_linear);\n        buffer[index + 1] = @intFromFloat(y_linear);\n        buffer[index + 2] = @intFromFloat(y_linear);\n    }\n}\n\ntry apply_image_filter(buffer[0..]);\n```\n:::\n\n\n\n\n## Saving the grayscale version of the image\n\nSince we have now the grayscale version of our image stored in our buffer object,\nwe need to encode this buffer object back into the \"PNG format\", and save the encoded data into\na new PNG file in our filesystem, so that we can access and see the grayscale version of our image\nthat was produced by our small program.\n\nTo do that, the `libspng` library help us once again by offering an \"encode data to PNG\" type of function,\nwhich is the `spng_encode_image()` function. But in order to \"encode data to PNG\" with `libspng`, we need\nto create a new context object. This new context object must use an \"encoder context\", which\nis identified by the enum value `SPNG_CTX_ENCODER`.\n\nThe `save_png()` function exposed below, summarises all the necessary steps to save the\ngrayscale version of our image into a new PNG file in the filesystem. By default, this\nfunction will save the grayscale image into a file named `pedro_pascal_filter.png` in the CWD.\n\nNotice in this code example that we are using the same image header object (`image_header`) that we have\ncollected previously with the `get_image_header()` function. Remember, this image header object\nis a C struct (`spng_ihdr`) that contains basic information about our PNG file, such as\nthe dimensions of the image, the color model used, etc.\n\nIf we wanted to save a very different image in this new PNG file, e.g. an image\nwith different dimensions, or, an image that uses a different color model, a different bit depth, etc.\nwe would have to create a new image header (`spng_ihdr`) object that describes the properties\nof this new image.\n\nBut we are essentially saving the same image that we have begin with here (the dimensions of\nthe image, the color model, etc. are all still the same). The only difference\nbetween the two images are the colors of the pixels, which are now \"shades of gray\".\nAs consequence, we can safely use the exact same image header data in this new PNG file.\n\n\n\n\n::: {.cell}\n\n```{.zig .cell-code}\nfn save_png(image_header: *c.spng_ihdr, buffer: []u8) !void {\n    const path = \"pedro_pascal_filter.png\";\n    const file_descriptor = c.fopen(path.ptr, \"wb\");\n    if (file_descriptor == null) {\n        return error.CouldNotOpenFile;\n    }\n    const ctx = (\n        c.spng_ctx_new(c.SPNG_CTX_ENCODER)\n        orelse unreachable\n    );\n    defer c.spng_ctx_free(ctx);\n    _ = c.spng_set_png_file(ctx, @ptrCast(file_descriptor));\n    _ = c.spng_set_ihdr(ctx, image_header);\n\n    const encode_status = c.spng_encode_image(\n        ctx,\n        buffer.ptr,\n        buffer.len,\n        c.SPNG_FMT_PNG,\n        c.SPNG_ENCODE_FINALIZE\n    );\n    if (encode_status != 0) {\n        return error.CouldNotEncodeImage;\n    }\n    if (c.fclose(file_descriptor) != 0) {\n        return error.CouldNotCloseFileDescriptor;\n    }\n}\n\ntry save_png(&image_header, buffer[0..]);\n```\n:::\n\n\nAfter we execute this `save_png()` function, we should have a new PNG file\ninside our CWD, named `pedro_pascal_filter.png`. If we open this PNG file,\nwe will see the same image exposed in @fig-pascal-gray.\n\n\n## Building our project\n\nNow that we have written the code, let's discuss how can we build/compile this project.\nTo do that, I'm going to create a `build.zig` file in the root directory of our project,\nand start writing the necessary code to compile the project, using the knowledge\nthat we have acquired from @sec-build-system.\n\n\nWe first create the build target for our executable file, that executes our\nZig code. Let's suppose that all of our Zig code was written into a Zig module\nnamed `image_filter.zig`. The `exe` object exposed in the build script below\ndescribes the build target for our executable file.\n\nSince we have used some C code from the `libspng` library in our Zig code,\nwe need to link our Zig code (which is in the `exe` build target) to both\nthe C Standard Library, and, to the `libspng` library. We do that, by calling\nthe `linkLibC()` and `linkSystemLibrary()` methods from our `exe` build target.\n\n\n::: {.cell}\n\n```{.zig .cell-code}\nconst std = @import(\"std\");\npub fn build(b: *std.Build) void {\n    const target = b.standardTargetOptions(.{});\n    const optimize = b.standardOptimizeOption(.{});\n    const exe = b.addExecutable(.{\n        .name = \"image_filter\",\n        .root_source_file = b.path(\"src/image_filter.zig\"),\n        .target = target,\n        .optimize = optimize,\n    });\n    exe.linkLibC();\n    // Link to libspng library:\n    exe.linkSystemLibrary(\"spng\");\n    b.installArtifact(exe);\n}\n```\n:::\n\n\nSince we are using the `linkSystemLibrary()` method, it means that the library\nfiles for `libspng` are searched in your system to be linked with the `exe` build target.\nIf you have not yet built and installed the `libspng` library into your system, this\nlinkage step will likely not work. Because it will not find the library files in your system.\n\nSo, just remember to install `libspng` in your system, if you want to build this project.\nHaving this build script above written, we can finally build our project by\nrunning the `zig build` command in the terminal.\n\n```bash\nzig build\n```\n",
    "supporting": [
      "13-image-filter_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}